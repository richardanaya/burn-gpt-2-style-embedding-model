# A GPT-2-like Embedding Model in Rust to Play with Training ğŸ¦€

This project implements a GPT-2 style transformer model for generating text embeddings, built from scratch in Rust using the [Burn](https://github.com/tracel-ai/burn) deep learning framework with WebGPU backend. It's designed as a learning resource for understanding transformer architecture and text embeddings.

## ğŸ¯ What This Project Does

This model takes sentences as input and converts them into high-dimensional vectors (embeddings) that capture semantic meaning. Similar sentences will have similar embeddings, making it useful for:

- **Semantic Search**: Find similar documents or sentences
- **Text Classification**: Use embeddings as features for ML models  
- **Clustering**: Group similar texts together
- **Similarity Analysis**: Measure how alike two pieces of text are

## ğŸ—ï¸ Architecture Overview

This implementation follows the GPT-2 117M parameter configuration:

```
ğŸ“Š Model Specifications:
â”œâ”€â”€ Transformer Blocks: 12
â”œâ”€â”€ Attention Heads: 12 per block
â”œâ”€â”€ Embedding Dimensions: 768
â”œâ”€â”€ Context Window: 1024 tokens
â”œâ”€â”€ Vocabulary Size: 50,257 (GPT-2 standard)
â””â”€â”€ Total Parameters: ~117M
```

### How Transformers Work (For Beginners)

Think of a transformer as a sophisticated pattern-matching system:

1. **Input Processing**: Text is converted to numbers (tokens)
2. **Attention Mechanism**: The model learns which words are important to each other
3. **Pattern Learning**: Multiple layers build up complex understanding
4. **Output Generation**: Creates a rich numerical representation (embedding)

## ğŸ§© Key Components

### 1. ğŸ“ Tokenizer (`src/tokenizer.rs`)
Converts text into numbers that the model can understand.

```rust
// Simple character-based tokenization for demo
let tokens = tokenizer.encode("Hello world", true)?;
// â†’ [72, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, ...]
```

#### Understanding Tokenization: From Text to Numbers

**Why Tokenization Matters:**
Neural networks can only work with numbers, not text. Tokenization is the crucial bridge that converts human-readable text into numerical sequences that transformers can process.

#### Real-World Tokenization vs Our Demo

**Our Demo Tokenizer (Simplified):**
- Maps each character directly to a number
- "Hello" â†’ [72, 101, 108, 108, 111] (ASCII values)
- Good for learning, but inefficient for real applications

**Real-World Tokenizers (BPE/SentencePiece):**
Modern tokenizers like GPT-2's use **Byte Pair Encoding (BPE)** which creates subword tokens:

```
"unhappiness" might become:
â”œâ”€â”€ "un" â†’ Token ID 1234
â”œâ”€â”€ "happy" â†’ Token ID 5678  
â””â”€â”€ "ness" â†’ Token ID 9012

Instead of 11 character tokens, we get just 3 meaningful subword tokens!
```

**Why Subword Tokenization is Powerful:**
- **Vocabulary Efficiency**: 50,257 tokens can represent millions of words
- **Unknown Word Handling**: Can tokenize words never seen in training
- **Semantic Preservation**: "happy" and "unhappy" share the "happy" token
- **Compression**: Reduces sequence length compared to character-level

#### Fragment Word Lists in Real Tokenizers

Real tokenizers maintain extensive vocabularies of word fragments:

```
Common BPE tokens include:
â”œâ”€â”€ Whole words: "the" (464), "and" (290), "of" (286)
â”œâ”€â”€ Prefixes: "re" (260), "un" (403), "pre" (661)  
â”œâ”€â”€ Suffixes: "ing" (278), "ed" (276), "ly" (306)
â”œâ”€â”€ Subwords: "tion" (357), "ness" (408), "able" (489)
â””â”€â”€ Characters: "a" (64), "e" (68), "!" (0)
```

**Token ID Assignment Process:**
1. **Text Input**: "The unhappy cat"
2. **BPE Segmentation**: ["The", "un", "happy", "cat"] 
3. **Lookup in Vocabulary**: [464, 403, 2995, 3797]
4. **Add Special Tokens**: [50256] + [464, 403, 2995, 3797] + [50256] (start/end)

#### From Token IDs to Transformer Input

Once we have token IDs, the transformer needs to convert them into rich numerical representations:

**Step 1: Token Embedding Lookup**
```rust
// Each token ID â†’ 768-dimensional vector
Token ID 464 ("The") â†’ [0.1, -0.3, 0.8, 0.2, ...] // 768 numbers
Token ID 403 ("un")  â†’ [-0.2, 0.5, 0.1, -0.7, ...] // 768 numbers  
Token ID 2995 ("happy") â†’ [0.4, 0.1, -0.2, 0.9, ...] // 768 numbers
```

**Step 2: Position Encoding**
```rust
// Add positional information so model knows word order
Position 0 â†’ [0.0, 0.1, 0.0, 0.3, ...] // 768 numbers
Position 1 â†’ [0.1, 0.0, 0.2, 0.1, ...] // 768 numbers
Position 2 â†’ [0.2, 0.3, 0.1, 0.0, ...] // 768 numbers

// Final input = Token Embedding + Position Embedding
"The" input = [0.1, -0.2, 0.8, 0.5, ...] // Ready for transformer!
```

**Step 3: What These Numbers Mean**
Each of the 768 dimensions potentially captures different linguistic features:
- Dimensions 1-100: Semantic meaning (animal, emotion, action)
- Dimensions 101-200: Syntactic role (noun, verb, adjective)  
- Dimensions 201-300: Grammatical features (tense, number, gender)
- Dimensions 301-768: Complex interactions and contextual nuances

This rich 768-dimensional representation gives the transformer everything it needs to understand and process the text!

### 2. ğŸ§  GPT-2 Model (`src/model.rs`)
The heart of the system - a transformer neural network with:

#### Transformer Block Architecture:
```
Input Text
    â†“
Token Embeddings + Position Embeddings
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer     â”‚  â† Repeated 12 times
â”‚ Block           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚Multi-Head   â”‚ â”‚  â† Attention: "What words relate to what?"
â”‚ â”‚Attention    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        +        â”‚  â† Residual Connection
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚Feed-Forward â”‚ â”‚  â† Processing: "What patterns do I see?"
â”‚ â”‚Network (MLP)â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚        +        â”‚  â† Residual Connection
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Layer Normalization
    â†“
Text Embeddings (768-dimensional vectors)
```

#### Multi-Head Attention: The Heart of Understanding

**What Multi-Head Attention Does:**
Multi-head attention allows the model to focus on different parts of the input simultaneously. Think of it as having multiple "attention spotlights" that can each focus on different aspects of language understanding.

**Why Multiple Heads?**
Each head can specialize in different types of relationships and linguistic patterns:

**ğŸ¯ Head Specialization Examples:**
- **Head 1**: Focuses on subject-verb relationships ("cat" â†” "sleeps")
- **Head 2**: Focuses on adjective-noun relationships ("big red" â†” "car")  
- **Head 3**: Focuses on local word dependencies ("very" â†” "quickly")
- **Head 4**: Focuses on long-range dependencies ("The" â†” "car" across sentence)
- **Head 5**: Focuses on syntactic patterns (detecting phrases and clauses)
- **Head 6**: Focuses on semantic similarity (synonyms and related concepts)
- **Head 7**: Focuses on positional patterns (beginning/end of sentences)
- **Head 8**: Focuses on rare/specific patterns (idioms, technical terms)
- **Head 9**: Focuses on temporal relationships ("before", "after", "while")
- **Head 10**: Focuses on causal relationships ("because", "therefore", "since")
- **Head 11**: Focuses on comparative relationships ("more", "less", "than")
- **Head 12**: Focuses on contextual disambiguation (resolving word meanings)

#### ğŸ” Concrete Example: "The big red car drove quickly"

Let's see how different attention heads might analyze this sentence:

**Head 1 (Subject-Verb Focus):**
```
"car" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º "drove" 
 â†‘                      â†‘
Strong attention: 0.9   â”‚
                       â”‚
The model learns: "car" is the subject performing "drove"
```

**Head 2 (Adjective-Noun Focus):**
```
"big" â”€â”€â”€â”€â–º "car" â—„â”€â”€â”€â”€ "red"
 â†‘           â†‘           â†‘
Attention:  0.8        0.8
           
The model learns: "big" and "red" both modify "car"
```

**Head 3 (Verb-Adverb Focus):**
```
"drove" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º "quickly"
  â†‘                       â†‘
Attention: 0.9            â”‚
                         â”‚
The model learns: "quickly" modifies how "drove" happened
```

**Head 4 (Long-Range Dependencies):**
```
"The" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º "car"
 â†‘                                   â†‘
Attention: 0.6                      â”‚
                                   â”‚
The model learns: "The" determines the specific "car" being discussed
```

**Head 5 (Syntactic Patterns):**
```
Noun Phrase Detection:
["The" + "big" + "red" + "car"] â†’ Attention cluster: 0.7-0.9
                                                      
Verb Phrase Detection:  
["drove" + "quickly"] â†’ Attention cluster: 0.8
                                          
The model learns: Grammatical phrase boundaries
```

**Head 6 (Semantic Relationships):**
```
"big" â—„â”€â”€â–º "red"     (both are descriptive attributes)
  â†‘         â†‘
Attention: 0.4       

"drove" â—„â”€â”€â–º "quickly" (action and manner are semantically linked)  
    â†‘         â†‘
Attention: 0.7

The model learns: Words with similar semantic roles
```

#### ğŸ’¡ How Multiple Heads Work Together

**Parallel Processing:**
All 12 heads process the sentence simultaneously, each creating their own attention patterns.

**Information Integration:**
```
Input: "The big red car drove quickly"
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Head 1: Subject-verb patterns       â”‚ â”€â”€â”
â”‚ Head 2: Adjective-noun patterns     â”‚   â”‚
â”‚ Head 3: Verb-adverb patterns        â”‚   â”‚  
â”‚ Head 4: Long-range dependencies     â”‚   â”‚ â†’ Combined Understanding
â”‚ Head 5: Syntactic structure         â”‚   â”‚
â”‚ Head 6: Semantic relationships      â”‚   â”‚
â”‚ ... (6 more heads)                  â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”€â”€â”˜
        â†“
Rich 768-dimensional representation capturing:
- Grammatical roles and relationships  
- Semantic meaning and context
- Syntactic structure and patterns
- Long and short-range dependencies
```

**Why 12 Heads Specifically?**
- **Computational Efficiency**: 768 dimensions Ã· 12 heads = 64 dimensions per head
- **Linguistic Coverage**: Enough heads to capture diverse language patterns  
- **Empirical Optimization**: Research shows 12 heads work well for this model size
- **Specialization vs Redundancy**: Balance between having specialized heads and backup capacity

**The Magic of Attention:**
Each head essentially asks: "For each word, which other words in the sentence are most important for understanding its meaning in this context?" The answers from all 12 heads combine to create a rich, contextual understanding that goes far beyond simple word-by-word processing.

#### ğŸ”§ How Head Results Get Merged Together

After all 12 heads have processed the input in parallel, their outputs need to be combined before feeding into the rest of the transformer. Here's exactly how this works:

**Step 1: Individual Head Outputs**
Each head produces attention-weighted representations for each token:
```
Head 1 output: [token1: 64-dim vector, token2: 64-dim vector, ...]
Head 2 output: [token1: 64-dim vector, token2: 64-dim vector, ...]
...
Head 12 output: [token1: 64-dim vector, token2: 64-dim vector, ...]
```

**Step 2: Concatenation**
The outputs from all heads are concatenated (joined together) for each token:
```
For each token position:
â”œâ”€â”€ Head 1 output: [aâ‚, aâ‚‚, ..., aâ‚†â‚„]    (64 dimensions)
â”œâ”€â”€ Head 2 output: [bâ‚, bâ‚‚, ..., bâ‚†â‚„]    (64 dimensions)  
â”œâ”€â”€ Head 3 output: [câ‚, câ‚‚, ..., câ‚†â‚„]    (64 dimensions)
â”‚   ...
â””â”€â”€ Head 12 output: [lâ‚, lâ‚‚, ..., lâ‚†â‚„]   (64 dimensions)
                    â†“
Concatenated: [aâ‚, aâ‚‚, ..., aâ‚†â‚„, bâ‚, bâ‚‚, ..., bâ‚†â‚„, câ‚, ..., lâ‚†â‚„]
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 768 dimensions total â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step 3: Linear Projection**
The concatenated 768-dimensional vector goes through a learned linear transformation:
```rust
// Pseudo-code representation
multi_head_output = concatenate([head1, head2, ..., head12])  // Shape: [768]
projected_output = linear_projection(multi_head_output)       // Shape: [768]
```

This linear projection (also called the "output projection") allows the model to:
- **Blend Information**: Learn optimal combinations of different head outputs
- **Reduce Redundancy**: Filter out redundant information between heads
- **Maintain Dimensionality**: Keep 768 dimensions for the residual connection

**Step 4: Residual Connection & Layer Norm**
```
Original input (768-dim)
       +                    â† Residual connection (preserves original info)
Projected multi-head output (768-dim)
       â†“
Layer Normalization         â† Stabilizes and normalizes the combined result
       â†“
Fed into Feed-Forward Network
```

**Visual Summary:**
```
Input: "The big red car"
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Multi-Head Attention Block       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”      â”‚ Each head: 768â†’64 dims
â”‚ â”‚Head1â”‚ â”‚Head2â”‚  ...  â”‚Head12â”‚      â”‚
â”‚ â”‚64dimâ”‚ â”‚64dimâ”‚       â”‚64dim â”‚      â”‚
â”‚ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”˜      â”‚
â”‚     â”‚       â”‚           â”‚          â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚ Concatenate: 12Ã—64â†’768
â”‚             â†“                      â”‚
â”‚    [768-dimensional vector]        â”‚
â”‚             â†“                      â”‚
â”‚    Linear Projection (768â†’768)     â”‚ Learn optimal blending
â”‚             â†“                      â”‚
â”‚    [768-dimensional output]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    + Residual + LayerNorm
         â†“
    Feed-Forward Network
```

**Why This Design Works:**
- **Parallel Specialization**: Each head can focus on different patterns independently
- **Information Preservation**: Concatenation keeps all specialized information
- **Adaptive Combination**: Linear projection learns how to best combine head outputs
- **Stable Training**: Residual connections prevent information loss during deep processing

This merging process ensures that the rich, specialized understanding from all 12 attention heads is effectively combined and passed forward through the transformer, creating the powerful contextual representations that make transformers so effective at understanding language!

#### Feed-Forward Network (MLP)
- **Purpose**: Processes the attended information to extract patterns
- **Architecture**: Linear â†’ GELU activation â†’ Linear â†’ Dropout
- **Size**: 4x expansion (768 â†’ 3072 â†’ 768 dimensions)

### 3. ğŸ“ Similarity Calculator (`src/similarity.rs`)
Compares embeddings using various mathematical approaches:

- **Cosine Similarity**: Measures angle between vectors (0-1, where 1 = identical)
- **Euclidean Distance**: Straight-line distance in high-dimensional space
- **Manhattan Distance**: Sum of absolute differences
- **Dot Product**: Raw similarity measure for normalized vectors

### 4. ğŸ“š Embedding Strategy: Mean Pooling from Final Layer

This implementation uses a specific strategy for creating sentence embeddings that's worth understanding in detail:

#### ğŸ¯ Mean Pooling Explained

**What is Mean Pooling?**
Mean pooling converts variable-length sequences of token embeddings into fixed-size sentence representations by averaging:

```
Input sentence: "The cat sleeps"
Token embeddings from final layer:
â”œâ”€â”€ "The":    [0.1, -0.2, 0.3, ...] (768 dims)
â”œâ”€â”€ "cat":    [0.5, 0.1, -0.1, ...] (768 dims)  
â””â”€â”€ "sleeps": [-0.2, 0.4, 0.2, ...] (768 dims)

Mean pooled result:
[(0.1+0.5-0.2)/3, (-0.2+0.1+0.4)/3, (0.3-0.1+0.2)/3, ...]
= [0.133, 0.1, 0.133, ...] (768 dims)
```

**Why Mean Pooling?**
- âœ… **Simplicity**: Easy to understand and implement
- âœ… **Robustness**: Works across different sentence lengths
- âœ… **No Training**: Requires no additional parameters
- âœ… **Stable**: Less sensitive to outlier tokens
- âš ï¸ **Equal Weighting**: Treats all words equally (pro and con)

#### ğŸ—ï¸ Why Final Layer vs Earlier Layers?

**Transformer Layers Build Progressive Understanding:**

```
Layer 1-3:   Basic syntax, part-of-speech, local word relationships
    â†“
Layer 4-8:   Complex grammar, named entities, coreference  
    â†“  
Layer 9-12:  Semantic meaning, context, abstract concepts
    â†“
Final Output: Rich semantic embeddings perfect for similarity
```

**Example: "The bank by the river was steep"**

- **Early Layers (1-3)**: Might mix financial and geographical meanings of "bank"
- **Middle Layers (4-8)**: Start to resolve meaning from nearby context words
- **Final Layers (9-12)**: Fully understand "bank" means riverbank from complete context

**Research Evidence:**
- Studies show semantic information peaks in final layers
- Sentence similarity tasks perform best with layers 10-12
- Earlier layers capture syntax; later layers capture meaning

#### ğŸ”„ Alternative Approaches We Could Use

**Different Pooling Strategies:**
```rust
// Max Pooling: Take maximum value for each dimension
embeddings.max_dim(1)  // Captures strongest signals

// CLS Token (BERT-style): Use special classification token  
embeddings.narrow(1, 0, 1)  // Take first token embedding

// Attention Pooling: Learn which tokens are most important
attention_weights * embeddings  // Requires training
```

**Different Layer Strategies:**
```rust
// Multi-layer combination: Use multiple layers
concat([layer9, layer10, layer11, layer12])

// Weighted layers: Learn optimal layer combination  
w1*layer9 + w2*layer10 + w3*layer11 + w4*layer12
```

**Why We Chose Final Layer + Mean Pooling:**
1. **Educational Clarity**: Easy to understand and explain
2. **Strong Baseline**: Proven effective across many tasks
3. **No Extra Training**: Works with any transformer model
4. **Research Foundation**: Widely used in academic work

This combination gives us embeddings that capture semantic meaning while being simple enough to understand and modify for learning purposes!

### 5. ğŸ’¾ Training Utilities (`src/training.rs`)
Handles loading and saving model weights in efficient binary format.

## ğŸ”„ How It Works: From Text to Embeddings

Let's trace through what happens when you input "The cat sat on the mat":

### Step 1: Tokenization
```
"The cat sat on the mat" â†’ [84, 104, 101, 32, 99, 97, 116, ...]
```

### Step 2: Embedding Lookup
```
Each token ID â†’ 768-dimensional vector
Position info â†’ Added to embeddings
```

### Step 3: Transformer Processing
```
For each of 12 transformer blocks:
  1. Multi-head attention analyzes token relationships
  2. Feed-forward network processes patterns
  3. Residual connections preserve information
  4. Layer normalization stabilizes training
```

### Step 4: Sentence Embedding
```
Token embeddings â†’ Averaged â†’ Single 768D sentence vector
```

This final vector captures the semantic meaning of the entire sentence!

## ğŸš€ Getting Started

### Prerequisites
- Rust 1.70+ 
- GPU with WebGPU support (or fallback to CPU)

### Installation
```bash
git clone https://github.com/richardanaya/burn-gpt-n-embedding-model
cd burn-gpt-n-embedding-model
cargo build --release
```

### Basic Usage

#### Get Embeddings for a Sentence
```bash
# Get embedding as JSON
cargo run -- embed --sentence "The quick brown fox jumps over the lazy dog"

# Output:
{
  "sentence": "The quick brown fox jumps over the lazy dog",
  "embedding": [0.1234, -0.5678, 0.9012, ...],
  "dimensions": 768
}
```

#### Calculate Similarity Between Sentences
```bash
# Compare two sentences
cargo run -- similarity \
  --sentence1 "The cat is sleeping" \
  --sentence2 "A feline is resting"

# Output:
Sentences:
  1: "The cat is sleeping"
  2: "A feline is resting"
Cosine Similarity: 0.8234 (0=different, 1=identical)
```

#### Get All Similarity Metrics
```bash
cargo run -- similarity \
  --sentence1 "I love programming" \
  --sentence2 "I hate coding" \
  --all-metrics

# Output:
Results:
  Cosine Similarity:    0.6543 (0=different, 1=identical)
  Euclidean Distance:   12.3456 (0=identical, higher=different)
  Manhattan Distance:   98.7654 (0=identical, higher=different)
  Dot Product Sim:      0.6234 (normalized vectors)
```

## ğŸ“ Understanding the Code Structure

```
src/
â”œâ”€â”€ main.rs          # CLI interface using CLAP
â”œâ”€â”€ lib.rs           # Module exports
â”œâ”€â”€ model.rs         # GPT-2 transformer implementation
â”œâ”€â”€ similarity.rs    # Vector similarity calculations
â”œâ”€â”€ tokenizer.rs     # Text-to-token conversion
â””â”€â”€ training.rs      # Model saving/loading utilities

data_sets/
â”œâ”€â”€ train.tsv        # Training data (sentence pairs + labels)
â”œâ”€â”€ dev.tsv          # Development/validation data
â””â”€â”€ test.tsv         # Test data
```

### Key Learning Points in Each File:

#### `model.rs` - The Neural Network
- **Gpt2Config**: Hyperparameters and model configuration
- **TransformerBlock**: Core attention + feed-forward pattern
- **Mlp**: Multi-layer perceptron with GELU activation
- **Gpt2Model**: Complete model assembly and forward pass

#### `similarity.rs` - Vector Mathematics
- **cosine_similarity()**: Understanding vector angles
- **euclidean_distance()**: Straight-line distance in high dimensions
- **SimilarityCalculator**: Practical similarity computation

## ğŸ“š Educational Concepts Demonstrated

### 1. **Embeddings vs One-Hot Encoding**
Traditional approaches represent words as sparse vectors (one 1, rest 0s). Embeddings create dense, meaningful representations where similar concepts cluster together.

### 2. **Attention Mechanism**
Instead of processing words sequentially, attention lets the model consider all words simultaneously and learn their relationships.

### 3. **Transfer Learning**
The same architecture used for text generation (GPT-2) can be adapted for embeddings by using intermediate representations.

### 4. **High-Dimensional Vector Spaces**
Text similarity becomes a geometric problem in 768-dimensional space where similar meanings cluster together.

### 5. **Transformer Architecture Benefits**
- **Parallelization**: All tokens processed simultaneously
- **Long-range dependencies**: Attention connects distant words
- **Contextual understanding**: Same word gets different embeddings in different contexts

## ğŸ“Š Data Format

Training data is in TSV (Tab-Separated Values) format:

```tsv
id	sentence1	sentence2	label
1	The cat is sleeping	A feline is resting	1
2	I love pizza	The weather is nice	0
3	Programming is fun	Coding is enjoyable	1
```

**Labels:**
- `1` = Similar sentences (should have similar embeddings)
- `0` = Different sentences (should have different embeddings)

This format enables training the model to learn meaningful semantic representations.

## ğŸ”¬ Advanced Features

### Multiple Similarity Metrics
Understanding different ways to measure vector similarity:

```rust
// Cosine similarity: Angle between vectors
let cos_sim = cosine_similarity(vec1, vec2);

// Euclidean distance: Straight-line distance  
let eucl_dist = euclidean_distance(vec1, vec2);

// Manhattan distance: Sum of coordinate differences
let manh_dist = manhattan_distance(vec1, vec2);
```

### Batch Processing
Efficiently process multiple sentences:

```rust
let embeddings = model.encode_sentences(input_batch);
let similarity_matrix = calculator.similarity_matrix(&sentences);
```

## ğŸ“ Learning Exercises

### Beginner
1. **Run the examples** and observe how similar sentences get higher similarity scores
2. **Experiment with different sentences** - what makes embeddings similar?
3. **Compare similarity metrics** - when do they agree/disagree?

### Intermediate  
1. **Modify the model architecture** - try different numbers of layers or dimensions
2. **Implement a better tokenizer** using subword tokenization
3. **Add training functionality** to learn from the TSV data

### Advanced
1. **Implement other similarity measures** (e.g., Pearson correlation)
2. **Add support for other transformer architectures** (BERT, RoBERTa)
3. **Create a web API** to serve embeddings at scale

## ğŸš€ Next Steps & Extensions

### Model Improvements
- **Better Tokenization**: Implement BPE or SentencePiece tokenizer
- **Training Loop**: Add actual training on the provided TSV data
- **Model Variants**: Support for different transformer sizes (345M, 762M, 1.5B)
- **Fine-tuning**: Domain-specific embedding training

### Technical Enhancements
- **Performance**: Optimize inference speed and memory usage
- **Deployment**: Add Docker containers and API endpoints
- **Monitoring**: Training metrics and embedding quality evaluation
- **Data Pipeline**: Efficient dataset loading and preprocessing

### Educational Resources
- **Interactive Notebooks**: Jupyter notebooks explaining concepts
- **Visualization**: Plot embeddings in 2D/3D space using dimensionality reduction
- **Comparison Studies**: Compare with other embedding models (Word2Vec, BERT)

## ğŸ› ï¸ Technical Stack

- **Language**: Rust ğŸ¦€
- **Deep Learning**: [Burn Framework](https://github.com/tracel-ai/burn)
- **Backend**: WebGPU (with CPU fallback)
- **CLI**: CLAP for command-line interface
- **Serialization**: SafeTensors for model weights
- **Testing**: Built-in Rust testing framework

## ğŸ“– Additional Resources

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper
- [GPT-2 Paper](https://openai.com/research/better-language-models) - OpenAI's GPT-2 research
- [Burn Documentation](https://burn-rs.github.io/) - Deep learning in Rust
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) - Visual guide to transformers

## ğŸ¤ Contributing

This project is designed for learning! Contributions that improve educational value are welcome:

- **Documentation improvements**
- **Code comments and explanations**  
- **Additional examples and tutorials**
- **Performance optimizations**
- **New similarity metrics or model variants**

## ğŸ“ License

This project is open source and available under the [MIT License](LICENSE).

---

**Happy Learning! ğŸ“** This implementation demonstrates how modern NLP models work under the hood. Experiment, modify, and most importantly - understand the beautiful mathematics that powers modern AI!
